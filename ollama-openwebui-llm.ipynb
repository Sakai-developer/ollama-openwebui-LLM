{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#下载open-webui包\n!git clone https://github.com/open-webui/open-webui.git\n%cd open-webui/\n\n# Copying required .env file\n%cp -RPp .env.example .env\n\n# Building Frontend Using Node\n!npm install\n!npm run build\n\n%cd ./backend\n!pip install -r requirements.txt -U\n\n#下载ollma\n!curl -fsSL https://ollama.com/install.sh | sh\n\n#下载内网穿透工具\n!npm install -g localtunnel","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#进入webui工作目录，前面已经进入了，这一步是为了重启内核后可直接运行这一步\n%cd /kaggle/working/open-webui/backend\nimport subprocess\nimport threading\nimport time\nimport socket\nimport urllib.request\n\nmodel_name = \"gemma2:27b\"\n\ndef iframe_thread(port):\n    while True:\n        time.sleep(0.5)\n        sock= socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        result = sock.connect_ex(('127.0.0.1', port))\n        if result == 0:\n            break\n        sock.close()\n\n        from colorama import Fore, Style\n    print (Fore.GREEN + \"\\nIP: \", Fore. RED, urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"), \"\\n\", Style. RESET_ALL)\n    p = subprocess.Popen([\"lt\", \"--port\", \"{}\".format(port)], stdout=subprocess.PIPE)\n    for line in p.stdout:\n        print(line.decode(), end='')\nprocess1 = subprocess.Popen(\"ollama serve\", shell=True)\nprocess2 = subprocess.Popen(f\"ollama pull {model_name}\", shell=True)\nprocess2.wait()\nprocess3 = subprocess.Popen(f\"ollama run {model_name}\", shell=True)\nprocess3.wait()\nthreading.Thread (target=iframe_thread, daemon=True, args=(8080,)).start()\n\n!bash start.sh","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}